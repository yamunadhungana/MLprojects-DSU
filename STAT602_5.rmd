---
title: "Modern Applied Statistics exercises from ISLR"
author: "Yamuna Dhungana"
output: 
    pdf_document:
        latex_engine: xelatex
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,warning=F,message=F)
```


1) Question 4.7.6 pg 170 
Suppose we collect data for a group of students in a statistics class with variables X1 = hours
studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce
estimated coefficient, $ˆ β0 = −6$, $ˆβ1 = 0.05$, $ˆβ2 = 1$.

a) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 
gets an A in the class.
    
  For the solution of this question, we simply plug the values of ˆβ0 , ˆβ1 and ˆβ2 in the
    equation:
    
$$\hat{p}(X) = \frac{e^{\hat\beta_0 + \hat\beta_1 {X_1}}}{(1 + e^{\hat\beta_0 + \hat\beta_1 {X_1}})}$$
    We have $ˆ β0 =−6$ ,$ˆβ1= 0.05$ and $ˆβ2 = 1$ 


$$\hat{p}(X) = \frac{e^{-6 + 0.05X_1 + X_2}}{(1 + e^{-6 + 0.05X_1 + X_2})} = \frac{e^{-6 + 0.05*40 + 1*3.5}}{(1 + e^{-6 + 0.05*40 + 1*3.5})} = 0.3775.$$
```{r, eval = TRUE}
cat("We can also write a function in R to calculate probability")
calculate_prob <- function(x1,x2){ p <- exp(-6 + 0.05*x1 + 1*x2); return( round(p/(1+p),4))}
calculate_prob(40,3.5)

```


b) How many hours would the student in part (a) need to study to have a 50% chance of getting an 
A in the class?

   The equation for predicted probability gives us:

$$\frac{e^{-6 + 0.05X_1 + 3.5}}{(1 + e^{-6 + 0.05X_1 + 3.5})} = 0.5$$
   from which, we get:

$$e^{-6 + 0.05X_1 + 3.5} = 1$$
   If we take log of both sides, we get:

$$X_1 = \frac{2.5}{0.05} = 50$$
```{r, eval = TRUE}
cat("We can also use the function to calculate this and get the answer")
hours <- seq(40,60,1)
probs <- mapply(hours, 3.5, FUN = calculate_prob)
names(probs) <- paste0(hours,"_hours")
probs
```

   Hence, to have 50 percent chance for securing A, the student has to study for 50 hours.
   
   

2) Continue from Homework \#3 \& 4 using the **Weekly** dataset from 4.7.10). Construct a model
(using the predictors chosen for previous homework) and fit this model using the `MclustDA` 
function from the **mclust** library. 

  i) Provide a summary of your model.

  - What is the best model selected by BIC? Report the Model Name and the BIC.
  (See[mclustModelNames]
  (https://www.rdocumentation.org/packages/mclust/versions/5.4/topics/mclustModelNames))

  - Report the true positive rate, true negative rate, training error, and test error. You can
    reuse the function written in Homework \# 3.
    
```{r,echo=FALSE,warning=FALSE}
library(ISLR)
library(mclust)
data("Weekly")

ex <- Weekly[,-8]
new_data <- c(which(Weekly$Year==2009), which(Weekly$Year==2010))
test_data <- ex[new_data,]
train <- ex[-new_data,]

# fitting the model with the predictors that I had chosen which is lag2
X.train = as.data.frame(train[,3]) ## keeping only Lag2
class.train = train$Direction

model1 <- MclustDA(X.train, class.train)
summary(model1)


pred_train=predict(model1, newdata=train[,3])$classification
## function to get overall accuracy, TPR and TNR
TPR_TNR=function(con){
  accuracy = round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100 - accuracy
  TPR=round(con[2,2]/(con[2,2]+con[1,2])*100,2)
  TNR=round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
  #return(list(overall_accuracy = accuracy,True_positive_rate = TPR,True_negative_rate = TNR))
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
}
# TPR_TNR(table(pred_train, class.train))
table_train <- TPR_TNR(table(pred_train, class.train))
colnames(table_train) <- "train_error"
table_train

# test error
predstest=predict(model1, newdata=test_data[,3])$classification
trueClass=test_data[,8]
# TPR_TNR(table(predstest,trueClass))
table_test <- TPR_TNR(table(predstest, trueClass))
colnames(table_test) <- "test_error"
table_test




#############################################

cat("## With all the variables")
X=as.data.frame(train[,-8]) 
model1a = MclustDA(X, class.train)
summary(model1a)
predstrain = predict(model1a, newdata=train[,-8])$classification
table_train2 <- TPR_TNR(table(predstrain, class.train))
colnames(table_train2) <- "train_error"
table_train2

predstest=predict(model1a, newdata=test_data[,-8])$classification
trueClass=test_data[,8]
table_test2 <- TPR_TNR(table(predstest,trueClass))
colnames(table_test2) <- "test_error"
table_test2



```
      
  
  From the previous work, I have selected Lag2 as the most significant variable. However, I am performing two models, one with only Lag2 and the other with all the variables. I only want to see how all the variables work. From the single variable, the model is variable variance (one-dimensional) with the two groups. And for the model with all the variables, the model is ellipsoidal, varying volume, shape, and orientation. The BIC of the single variable model is more than the BIC with all the variables. I have also made the table for both the test and train for these models. 
    

  ii) Repeat the `MclustDA` analysis, but this time specify `modelType = "EDDA"`. Provide a summary of this model.
    * What is the best model using BIC as the model selection criteria? 

    * Report the true positive rate, true negative rate, training error, and test error. You can reuse the function written in Homework \# 3.
    
```{r,echo=FALSE,warning=FALSE}

X=as.data.frame(train[,-8])
class <- train$Direction
model2=MclustDA(X, class, modelType = "EDDA")
summary(model2)

## train error
preds.train=predict(model2, newdata=train[,-8])$classification
table_train3 <- TPR_TNR(table(preds.train,class))
colnames(table_train3) <- "train_error"
table_train3


## test error
predstest=predict(model2, newdata=test_data[,-8])$classification
trueClass=test_data[,8]
table_test3 <- TPR_TNR(table(predstest,trueClass))
colnames(table_test3) <- "test_error"
table_test3



```
     
  I have fitted the model with all the variables and with a single variable. However, the model with the single variable could not converge. I have made the table for the train and the test errors for the model. From the documentation of Mclust, it was found that specifying “EDDA” as the model type, we force the model to have a single component in each class with the same covariance structure. We see that the single component had an ellipsoidal, equal orientation (VVE) structure.



  iii) Compare the results with Homework \#3 \& 4. Which method performed the best? Justify your answer. *Present your results in a well formatted table; include the previous methods and their corresponding rates.*
    
```{r,echo=FALSE,warning=FALSE}

library(knitr)
tablecom <- as.data.frame(cbind(table_train, table_test))
knitr::kable(tablecom, digits = 3,
             caption = "MsclustDA Test Accuracy with single variable")

tablecom2 <- as.data.frame(cbind(table_train2, table_test2,table_train3,table_test3))
knitr::kable(tablecom2, digits = 3,
             caption = "MsclustDA Test Accuracy with all variables")

tablecom2a <- as.data.frame(cbind(table_train3,table_test3))
knitr::kable(tablecom2a, digits = 3,
             caption = "MsclustDA with EDDA Test Accuracy with all variables")


# for my previous work
# Logistic regression

fit_log2 <- glm(Direction~ Lag2, data = train, family= binomial)
#summary(fit_log2)

do.confusion=function(Th.hold,model,data){
  preds=rep("Down",dim(data)[1])
  vals=predict(model,newdata=data,type="response")
  for(i in 1:dim(data)[1]){
    if(vals[i]>=Th.hold){
      preds[i]="Up"
    }
  }
  con=table(preds,data$Direction)
  accuracy = round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100 - accuracy
  TPR=round(con[2,2]/(con[2,2]+con[1,2])*100,2)
  TNR=round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
  #return(list(overall_accuracy = accuracy,True_positive_rate = TPR,True_negative_rate = TNR))
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
  
}
# Train error
table_train4 <- do.confusion(0.5,fit_log2,train)
colnames(table_train4) <- "Train_error"

# Test error
table_test4 <- do.confusion(0.5,fit_log2,test_data)
colnames(table_test4) <- "Test_error"

# Combined error for logistic error
tablecom3 <- as.data.frame(cbind(table_train4, table_test4))
knitr::kable(tablecom3, digits = 3,
             caption = "Logreg Accuracy measures with single variable")


# FOr lda
library(MASS)
fit_lda = lda(Direction ~ Lag2, data = train)

# For qda
fit_qda <- qda(Direction~Lag2,data=train)

confusion_lqda =function(model,data){
  preds=(predict(model,newdata=data,type="response"))$class
  vals=predict(model,newdata=data,type="response")
  con=table(preds,data$Direction)
  accuracy = (round(sum(preds==data$Direction)/dim(data)[1]*100,2))
  TPR = (round(con[2,2]/(con[2,2]+con[1,2])*100,2))
  TNR = round(con[1,1]/(con[1,1]+con[2,1])*100,2)
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
  
}
# Train error
table_train5 <- confusion_lqda(fit_lda, train)
colnames(table_train5) <- "Train_error"

# Test error
table_test5 <- confusion_lqda(fit_lda, test_data)
colnames(table_test5) <- "Test_error"

# Combined error for lda error
tablecom4 <- as.data.frame(cbind(table_train5, table_test5))
knitr::kable(tablecom4, digits = 3,
             caption = "LDA Accuracy measures with single variable")


#############################################################
# Train error
table_train6 <- confusion_lqda(fit_qda, train)
colnames(table_train6) <- "Train_error"

# Test error
table_test6 <- confusion_lqda(fit_qda, test_data)
colnames(table_test6) <- "Test_error"

# Combined error for Qda error
tablecom5 <- as.data.frame(cbind(table_train6, table_test6))
knitr::kable(tablecom5, digits = 3,
             caption = "QDA Accuracy measures with single variable")



###########################################################################

do.confusionknn =function(model,trues){
  con=table(model,trues)
  accuracy = (round(((con[1,1]+con[2,2])/sum(con))*100,2))
  TPR = (round(con[2,2]/(con[2,2]+con[1,2])*100,2))
  TNR = round(con[1,1]/(con[1,1]+con[2,1])*100)
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
  
}

attach(Weekly)
# head(Weekly)
k_tdata = (Year < 2009)
knn_train = as.matrix(Lag2[k_tdata])
knn_test = as.matrix(Lag2[!k_tdata])
train_class = Direction[k_tdata]

library(class)
fit_knn <- knn(knn_train, knn_test, cl=train_class, k = 1)


# Test error
table_test7 <- do.confusionknn(fit_knn, test_data$Direction)
colnames(table_test7) <- "Test_error"

# Combined error for Qda error
tablecom6 <- as.data.frame(cbind(table_test7))
knitr::kable(tablecom6, digits = 3,
             caption = "KNN Accuracy measures with single variable")


```
      
    
  Here, I have made some tables for all the methods that we fitted on this as well as previous. By looking at the table, we can find out that the accuracy of the test data varies from 62.50 to 46.15. Likewise, training accuracy varies from 64.47 and 50.0. The accuracy of the logistic regression is highly accurate besides logistic regression, LDA also has a high accuracy rate.
    
    
    
  iv) From the original model variables, construct a new set of variables, fit a model using `MclustDA` and repeat i-iii. *Hint: new variables may be interactions, polynomials, and/or splines.* Do these new variables give an improvement in error rates compared to previous models? Explain how the new variables were constructed.
    
    
```{r,echo=FALSE,warning=FALSE}
# formula1=Direction~Lag1+Lag2
# formula2=Direction~Lag1+Lag2+Lag1*Lag2
# formula3=Direction~Lag2+I(Lag2^2)

exx <- Weekly[,-8]
# Keeping only lag1 and lag2
X1 = as.data.frame(exx[,2:3])
lag12 <- (exx$Lag1 * ex$Lag2)
X2 = cbind(X1, lag12)
direc <- exx$Direction
Xsq = (exx$Lag2)^2
finaltab <- cbind(X2,Xsq,direc)

new_data2 <- c(which(Weekly$Year==2009), which(Weekly$Year==2010))
tst_data <- finaltab[new_data2,]
trn_data <- finaltab[-new_data2,]
trn.class <- trn_data$direc




## MclustDA
modd1 <- MclustDA(trn_data[,1:2], trn.class)
summary(modd1)

modd2 <- MclustDA(trn_data[,1:3], trn.class)
summary(modd2)

modd3 <- MclustDA(trn_data[,c(2,4)], trn.class)
summary(modd3)

###################################################

## function to get overall accuracy, TPR and TNR
TPR_TNR_iv=function(con){
  accuracy = round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100 - accuracy
  TPR=round(con[2,2]/(con[2,2]+con[1,2])*100,2)
  TNR=round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
  #return(list(overall_accuracy = accuracy,True_positive_rate = TPR,True_negative_rate = TNR))
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
}
pred_train1 = predict(modd1, newdata=trn_data[,1:2])$classification
table_train_1 <- TPR_TNR_iv(table(pred_train1, trn.class))
colnames(table_train_1) <- "tr.error modd1"
#table_train_1

# test error
predstest=predict(modd1, newdata=tst_data[,1:2])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_1 <- TPR_TNR_iv(table(predstest, tClass))
colnames(table_test_1) <- "tt.error modd1"
#table_test_1



# For second mmodel
# Train error
pred_train2 = predict(modd2, newdata=trn_data[,1:3])$classification
table_train_2 <- TPR_TNR_iv(table(pred_train2, trn.class))
colnames(table_train_2) <- "tr.error modd2"
#table_train_2

# test error
predstest2=predict(modd2, newdata=tst_data[,1:3])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_2 <- TPR_TNR_iv(table(predstest2, tClass))
colnames(table_test_2) <- "tt.error modd2"
#table_test_2



# For third model
# Train error
pred_train3 = predict(modd3, newdata=trn_data[,c(2,4)])$classification
table_train_3 <- TPR_TNR_iv(table(pred_train3, trn.class))
colnames(table_train_3) <- "tr.error modd3"
#table_train_3

# test error
predstest3=predict(modd3, newdata=tst_data[,c(2,4)])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_3 <- TPR_TNR_iv(table(predstest3, tClass))
colnames(table_test_3) <- "tt.error modd3"
#table_test_3


tablecombined <- as.data.frame(cbind(table_train_1,table_test_1,table_train_2,table_test_2,
                                     table_train_3,table_test_3))
knitr::kable(tablecombined, digits = 3,
             caption = "Accuracy measures using MclustDA")




################################################################################

## MclustDAwith EDDA
edmod1 <- MclustDA(trn_data[,1:2], trn.class, modelType = "EDDA")
summary(edmod1)

edmod2 <- MclustDA(trn_data[,1:3], trn.class, modelType = "EDDA")
summary(edmod2)

edmod3 <- MclustDA(trn_data[,c(2,4)], trn.class, modelType = "EDDA")
summary(edmod3)



# ## function to get overall accuracy, TPR and TNR
# TPR_TNR_iv=function(con){
#   accuracy = round(100*(con[1,1]+con[2,2])/sum(con),2) ## error rate is 100 - accuracy
#   TPR=round(con[2,2]/(con[2,2]+con[1,2])*100,2)
#   TNR=round(con[1,1]/(con[1,1]+con[2,1])*100,2) ## FPR=100-TNR
#   #return(list(overall_accuracy = accuracy,True_positive_rate = TPR,True_negative_rate = TNR))
#   return(as.data.frame(rbind(accuracy,TPR,TNR)))
# }
pred_traine1e = predict(edmod1, newdata=trn_data[,1:2])$classification
table_train_1e <- TPR_TNR_iv(table(pred_traine1e, trn.class))
colnames(table_train_1e) <- "tr.error.1ed"
# table_train_1e

# test error
predsteste=predict(edmod1, newdata=tst_data[,1:2])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_1e <- TPR_TNR_iv(table(predsteste, tClass))
colnames(table_test_1e) <- "tt.error.1ed"
#table_test_1



# For second mmodel
# Train error
pred_train2e = predict(edmod2, newdata=trn_data[,1:3])$classification
table_train_2e <- TPR_TNR_iv(table(pred_train2e, trn.class))
colnames(table_train_2e) <- "tr.error.2ed"
#table_train_2

# test error
predstest2e=predict(edmod2, newdata=tst_data[,1:3])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_2e <- TPR_TNR_iv(table(predstest2e, tClass))
colnames(table_test_2e) <- "tt.error.2ed"
#table_test_2



# For third model
# Train error
pred_train3e = predict(edmod3, newdata=trn_data[,c(2,4)])$classification
table_train_3e <- TPR_TNR_iv(table(pred_train3e, trn.class))
colnames(table_train_3e) <- "tr.error.3ed"
#table_train_3

# test error
predstest3e = predict(edmod3, newdata=tst_data[,c(2,4)])$classification
tClass=tst_data[,5]
# TPR_TNR(table(predstest,trueClass))
table_test_3e <- TPR_TNR_iv(table(predstest3e, tClass))
colnames(table_test_3e) <- "tt.error.3ed"
#table_test_3


tablecombined2 <- as.data.frame(cbind(table_train_1e,table_test_1e,table_train_2e,table_test_2e,
                                     table_train_3e,table_test_3e))
knitr::kable(tablecombined2, digits = 3,
             caption = "Accuracy measures using MclustDA EDDA")


##########################################################################################
# Now performing LDA and QDA
formula1=direc~Lag1+Lag2
formula2=direc~Lag1+Lag2+lag12  # Direction~Lag1+Lag2+Lag1*Lag2
formula3=direc~Lag2+Xsq         # Direction~Lag2+I(Lag2^2)


confusion_lqda =function(model,data){
  preds=(predict(model,newdata=data,type="response"))$class
  vals=predict(model,newdata=data,type="response")
  con=table(preds,data$direc)
  accuracy = (round(sum(preds==data$direc)/dim(data)[1]*100,2))
  TPR = (round(con[2,2]/(con[2,2]+con[1,2])*100,2))
  TNR = round(con[1,1]/(con[1,1]+con[2,1])*100,2)
  return(as.data.frame(rbind(accuracy,TPR,TNR)))
  
}

# First model 
fit_lda1=lda(formula1,data=trn_data)

# Train error
table_trainlda1 <- confusion_lqda(fit_lda1, trn_data)
colnames(table_trainlda1) <- "Trn_err.lda1"

# Test error
table_testlda1 <- confusion_lqda(fit_lda1, tst_data)
colnames(table_testlda1) <- "Tt_err.lda1"


# Second model 
fit_lda2=lda(formula2,data=trn_data)

# Train error
table_trainlda2 <- confusion_lqda(fit_lda2, trn_data)
colnames(table_trainlda2) <- "Trn_err.lda2"

# Test error
table_testlda2 <- confusion_lqda(fit_lda2, tst_data)
colnames(table_testlda2) <- "Tt_err.lda2"


# Third model 
fit_lda3=lda(formula3,data=trn_data)

# Train error
table_trainlda3 <- confusion_lqda(fit_lda3, trn_data)
colnames(table_trainlda3) <- "Trn_err.lda3"

# Test error
table_testlda3 <- confusion_lqda(fit_lda3, tst_data)
colnames(table_testlda3) <- "Tt_err.lda3"


# Combining table 

tablecombined3 <- as.data.frame(cbind(table_trainlda1,table_testlda1,table_trainlda2,table_testlda2,
                                     table_trainlda3,table_testlda3))
knitr::kable(tablecombined3, digits = 3,
             caption = "Accuracy measures using LDA")




#############################################################################

# First model 
fit_qda1=qda(formula1,data=trn_data)

# Train error
table_trainqda1 <- confusion_lqda(fit_qda1, trn_data)
colnames(table_trainqda1) <- "Trn_err.qda1"

# Test error
table_testqda1 <- confusion_lqda(fit_qda1, tst_data)
colnames(table_testqda1) <- "Tt_err.qda1"


# Second model 
fit_qda2=qda(formula2,data=trn_data)

# Train error
table_trainqda2 <- confusion_lqda(fit_qda2, trn_data)
colnames(table_trainqda2) <- "Trn_err.qda2"

# Test error
table_testqda2 <- confusion_lqda(fit_qda2, tst_data)
colnames(table_testqda2) <- "Tt_err.qda2"


# Third model 
fit_qda3=qda(formula3,data=trn_data)

# Train error
table_trainqda3 <- confusion_lqda(fit_qda3, trn_data)
colnames(table_trainqda3) <- "Trn_err.qda3"

# Test error
table_testqda3 <- confusion_lqda(fit_qda3, tst_data)
colnames(table_testqda3) <- "Tt_err.qda3"



# Combining table 

tablecombined4 <- as.data.frame(cbind(table_trainqda1,table_testqda1,table_trainqda2,table_testqda2,
                                     table_trainqda3,table_testqda3))
knitr::kable(tablecombined4, digits = 3,
             caption = "Accuracy measures using QDA")



```

 
  I have created three models, `Direction~Lag1+Lag2`,`Direction~Lag1+Lag2+Lag1*Lag2`      `Direction~Lag2+I(Lag2^2)` for the I have fitted these three models with all the previous models that we have learned. Both models with MclustDA have a model spherical, unequal volume with the 3 groups for down and two for the up. Likewise, the second model has ellipsoidal, equal shape for down and  Up with the ellipsoidal, varying volume, shape, and orientation with 5 groups. Additionally, the third model is ellipsoidal, varying volume, shape, and orientation with 5 groups. For the first model of EDDA, there is one group with a spherical, equal volume model. The second and third models have ellipsoidal, varying volume, shape, and orientation models with one group.I have made the table for all      the model's accuracy in a combined table that represents the test and the training error. 






